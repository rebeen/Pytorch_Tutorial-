{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datas\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_dataset = datas.MNIST(root='./data',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "test_dataset = datas.MNIST(root='./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dataset iterable\n",
    "batch_size= 100\n",
    "n_iters=3000\n",
    "num_epochs= int(n_iters/ (len(train_dataset)/ batch_size))\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader= torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, ouput_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        #hidden dimensions\n",
    "        self.hidden_dim= hidden_dim\n",
    "        \n",
    "        # Number of hidden layers \n",
    "        self.layer_dim= layer_dim\n",
    "        \n",
    "        #building our RNN\n",
    "        # batch first =true causes input/output tensors to be of shape\n",
    "        #(batch_dim, seq_dim, input_dim)\n",
    "        \n",
    "        self.lstm= nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        #readout layer\n",
    "        self.fc=nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        \n",
    "        c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        \n",
    "        #one time step\n",
    "        out, (hn,cn)= self.lstm(x,( h0,c0))\n",
    "        \n",
    "        #index hidden state of last time step\n",
    "        # out.size() --> 100 ,28,100\n",
    "        out= self.fc(out[:,-1,:])  # we only want the last dimension \n",
    "        #out.size()--> 100,10  \n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate model class\n",
    "input_dim=28\n",
    "hidden_dim=100\n",
    "layer_dim=1  # we can incrase the layer to have more hidden layer\n",
    "output_dim=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LSTMModel(input_dim, hidden_dim, layer_dim,output_dim)\n",
    "\n",
    "criterion=  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.00000001\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rebeen/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. loss: 2.2964017391204834. accuracy: 4\n",
      "Iteration: 1000. loss: 2.3134865760803223. accuracy: 4\n",
      "Iteration: 1500. loss: 2.3141684532165527. accuracy: 4\n",
      "Iteration: 2000. loss: 2.31162166595459. accuracy: 4\n"
     ]
    }
   ],
   "source": [
    "seq_dim=28\n",
    "\n",
    "iter=0\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    for i ,(images, labels) in enumerate(train_loader):\n",
    "        images= Variable(images.view(-1,seq_dim,input_dim))\n",
    "        labels= Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad() #clear gradient w.r.t parameters\n",
    "        \n",
    "        outputs= model(images)  # 100 to 10 forward pass to get output\n",
    "        \n",
    "        loss= criterion( outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "#         clipped_lr = lr * clip_gradient(model, clip)\n",
    "#         for p in model.parameters():\n",
    "#             p.data.add_(-clipped_lr, p.grad.data)\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter +=1\n",
    "        \n",
    "        if iter %500 == 0:\n",
    "        \n",
    "        #calculate accuracy\n",
    "            correct = 0\n",
    "            total=0\n",
    "            for images ,labels in test_loader:\n",
    "                    images =Variable(images.view(-1,seq_dim,input_dim))\n",
    "                    outputs= model(images)\n",
    "\n",
    "                    _,predicted= torch.max(outputs.data,1)\n",
    "                    total += labels.size(0)\n",
    "                    correct +=(predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100* correct / total\n",
    "\n",
    "            print('Iteration: {}. loss: {}. accuracy: {}'.format(iter,loss.item(),accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
