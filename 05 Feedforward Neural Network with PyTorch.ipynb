{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datas\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining input size, hidden layer size, output size and batch size respectively\n",
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input and target tensors (data)\n",
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], \n",
    "[1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2886,  0.7175,  2.1371,  0.4113,  0.1662,  1.5883,  2.0347, -1.1006,\n",
       "         -0.0509,  1.2631],\n",
       "        [-1.0126, -0.2938,  0.5533,  0.1124, -1.0568,  1.3098,  0.4262, -0.4173,\n",
       "          0.2816, -0.5375],\n",
       "        [ 1.4169, -0.0797,  1.1612,  0.0687, -0.1562, -1.1676,  2.0485,  1.8608,\n",
       "          2.0300,  1.1910],\n",
       "        [ 0.4996,  2.7504,  0.3877,  0.9012,  0.3156,  0.8128, -0.9790, -0.6299,\n",
       "         -1.9258, -0.2414],\n",
       "        [-1.8081,  0.5160,  0.2266, -1.5305, -0.8709,  0.5109,  1.0504,  0.8743,\n",
       "          0.3495, -0.0736],\n",
       "        [ 0.0427, -1.3355,  1.4703,  0.1165, -0.4424,  0.7350,  1.6205, -0.2679,\n",
       "          0.0206, -0.5124],\n",
       "        [-0.1353, -0.0570,  0.2198, -1.3116,  1.5062,  0.7166, -0.0453, -1.0480,\n",
       "          1.1839,  0.2257],\n",
       "        [-1.6856, -0.3841,  0.0731, -0.0125,  0.3736, -1.0028,  0.9416,  1.7250,\n",
       "          0.8049,  0.8265],\n",
       "        [ 0.3936, -0.8972, -0.4180,  0.2298, -0.0729, -1.1947, -0.7851, -0.5421,\n",
       "          1.8242, -0.8861],\n",
       "        [-2.2073, -0.9643, -0.8648,  0.5960, -0.2093, -1.0802,  0.2087, -1.1960,\n",
       "         -0.8123,  1.1481]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating model\n",
    "model=nn.Sequential(nn.Linear(n_in,n_h),\n",
    "                    nn.ReLU(),\n",
    "                     nn.Linear(n_h,n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss 0.2422918826341629\n",
      "epoch: 1 loss 0.24222949147224426\n",
      "epoch: 2 loss 0.2421671748161316\n",
      "epoch: 3 loss 0.24210496246814728\n",
      "epoch: 4 loss 0.24204282462596893\n",
      "epoch: 5 loss 0.24198079109191895\n",
      "epoch: 6 loss 0.24191881716251373\n",
      "epoch: 7 loss 0.24185694754123688\n",
      "epoch: 8 loss 0.24179516732692719\n",
      "epoch: 9 loss 0.24173346161842346\n",
      "epoch: 10 loss 0.2416718453168869\n",
      "epoch: 11 loss 0.24161028861999512\n",
      "epoch: 12 loss 0.24154885113239288\n",
      "epoch: 13 loss 0.24148747324943542\n",
      "epoch: 14 loss 0.24142619967460632\n",
      "epoch: 15 loss 0.241364985704422\n",
      "epoch: 16 loss 0.24130384624004364\n",
      "epoch: 17 loss 0.24124279618263245\n",
      "epoch: 18 loss 0.24118183553218842\n",
      "epoch: 19 loss 0.24112093448638916\n",
      "epoch: 20 loss 0.24106012284755707\n",
      "epoch: 21 loss 0.24099937081336975\n",
      "epoch: 22 loss 0.2409387081861496\n",
      "epoch: 23 loss 0.2408781498670578\n",
      "epoch: 24 loss 0.2408176213502884\n",
      "epoch: 25 loss 0.24075718224048615\n",
      "epoch: 26 loss 0.24069683253765106\n",
      "epoch: 27 loss 0.24063652753829956\n",
      "epoch: 28 loss 0.2405763417482376\n",
      "epoch: 29 loss 0.24051620066165924\n",
      "epoch: 30 loss 0.24045610427856445\n",
      "epoch: 31 loss 0.24039612710475922\n",
      "epoch: 32 loss 0.24033619463443756\n",
      "epoch: 33 loss 0.24027636647224426\n",
      "epoch: 34 loss 0.24021656811237335\n",
      "epoch: 35 loss 0.2401568442583084\n",
      "epoch: 36 loss 0.24009719491004944\n",
      "epoch: 37 loss 0.24003762006759644\n",
      "epoch: 38 loss 0.2399781197309494\n",
      "epoch: 39 loss 0.23991867899894714\n",
      "epoch: 40 loss 0.23985929787158966\n",
      "epoch: 41 loss 0.23980000615119934\n",
      "epoch: 42 loss 0.2397407591342926\n",
      "epoch: 43 loss 0.23968157172203064\n",
      "epoch: 44 loss 0.23962244391441345\n",
      "epoch: 45 loss 0.23956340551376343\n",
      "epoch: 46 loss 0.23950441181659698\n",
      "epoch: 47 loss 0.23944547772407532\n",
      "epoch: 48 loss 0.23938661813735962\n",
      "epoch: 49 loss 0.2393278181552887\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "\n",
    "for epoch in range(50):\n",
    "    y_pred=model(x)   # Forward pass: Compute predicted y by passing x to the model\n",
    "\n",
    "    loss=criterion(y_pred,y)    # Compute and print loss\n",
    "\n",
    "    print(\"epoch:\",epoch, 'loss',loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()   # Zero gradients, perform a backward pass, and update the weights.\n",
    "\n",
    "    \n",
    "    loss.backward()   # perform a backward pass (backpropagation)\n",
    "\n",
    "    optimizer.step()   # Update the parameters\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "trin_dataset = datas.MNIST(root='./data',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "test_data = datas.MNIST(root='./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset iterable \n",
    "batch_size = 100\n",
    "epochs = 100 \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trin_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class):\n",
    "        super(FeedForward, self).__init__()\n",
    "#         linear functions\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         non_linearity \n",
    "        self.sigmoied = nn.Sigmoid()\n",
    "#         linear function\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         linear functions\n",
    "        out = self.fc1(x)\n",
    "#         Non_linearity\n",
    "        out = self.sigmoied(out)\n",
    "#         linear function\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "itter = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = FeedForward(input_dim, hidden_dim, output_dim).cuda()\n",
    "else:\n",
    "    model = FeedForward(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of FeedForward(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (sigmoied): Sigmoid()\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "\n",
    "print(list(model.parameters())[0].size())\n",
    "print(list(model.parameters())[1].size())\n",
    "print(list(model.parameters())[2].size())\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itteration  500 accracy tensor(86) loss 0.7421950697898865\n",
      "itteration  1000 accracy tensor(89) loss 0.3847352862358093\n",
      "itteration  1500 accracy tensor(90) loss 0.25892868638038635\n",
      "itteration  2000 accracy tensor(91) loss 0.3743802011013031\n",
      "itteration  2500 accracy tensor(91) loss 0.3227367699146271\n",
      "itteration  3000 accracy tensor(92) loss 0.24755211174488068\n",
      "itteration  3500 accracy tensor(92) loss 0.1717093288898468\n",
      "itteration  4000 accracy tensor(92) loss 0.2977719008922577\n",
      "itteration  4500 accracy tensor(92) loss 0.12789393961429596\n",
      "itteration  5000 accracy tensor(93) loss 0.2943492829799652\n",
      "itteration  5500 accracy tensor(93) loss 0.18212474882602692\n",
      "itteration  6000 accracy tensor(93) loss 0.2248486876487732\n",
      "itteration  6500 accracy tensor(93) loss 0.18854670226573944\n",
      "itteration  7000 accracy tensor(93) loss 0.20522858202457428\n",
      "itteration  7500 accracy tensor(94) loss 0.12032351642847061\n",
      "itteration  8000 accracy tensor(94) loss 0.17394286394119263\n",
      "itteration  8500 accracy tensor(94) loss 0.3090498745441437\n",
      "itteration  9000 accracy tensor(94) loss 0.208602175116539\n",
      "itteration  9500 accracy tensor(94) loss 0.2269008308649063\n",
      "itteration  10000 accracy tensor(94) loss 0.09881091862916946\n",
      "itteration  10500 accracy tensor(94) loss 0.173516184091568\n",
      "itteration  11000 accracy tensor(94) loss 0.2644399106502533\n",
      "itteration  11500 accracy tensor(95) loss 0.14098289608955383\n",
      "itteration  12000 accracy tensor(95) loss 0.0731540396809578\n",
      "itteration  12500 accracy tensor(95) loss 0.2564254701137543\n",
      "itteration  13000 accracy tensor(95) loss 0.060529328882694244\n",
      "itteration  13500 accracy tensor(95) loss 0.20744666457176208\n",
      "itteration  14000 accracy tensor(95) loss 0.09871527552604675\n",
      "itteration  14500 accracy tensor(95) loss 0.26294052600860596\n",
      "itteration  15000 accracy tensor(95) loss 0.08041355013847351\n",
      "itteration  15500 accracy tensor(95) loss 0.11595284193754196\n",
      "itteration  16000 accracy tensor(95) loss 0.09852354228496552\n",
      "itteration  16500 accracy tensor(95) loss 0.08639582991600037\n",
      "itteration  17000 accracy tensor(95) loss 0.2250523716211319\n",
      "itteration  17500 accracy tensor(95) loss 0.11035700887441635\n",
      "itteration  18000 accracy tensor(96) loss 0.09770345687866211\n",
      "itteration  18500 accracy tensor(96) loss 0.057074423879384995\n",
      "itteration  19000 accracy tensor(96) loss 0.06962645798921585\n",
      "itteration  19500 accracy tensor(96) loss 0.11664574593305588\n",
      "itteration  20000 accracy tensor(96) loss 0.16910377144813538\n",
      "itteration  20500 accracy tensor(96) loss 0.16151008009910583\n",
      "itteration  21000 accracy tensor(96) loss 0.1283087283372879\n",
      "itteration  21500 accracy tensor(96) loss 0.04192983731627464\n",
      "itteration  22000 accracy tensor(96) loss 0.11002204567193985\n",
      "itteration  22500 accracy tensor(96) loss 0.0436108261346817\n",
      "itteration  23000 accracy tensor(96) loss 0.12534010410308838\n",
      "itteration  23500 accracy tensor(96) loss 0.0729239359498024\n",
      "itteration  24000 accracy tensor(96) loss 0.05581308528780937\n",
      "itteration  24500 accracy tensor(96) loss 0.062243618071079254\n",
      "itteration  25000 accracy tensor(96) loss 0.05243426188826561\n",
      "itteration  25500 accracy tensor(96) loss 0.04993148148059845\n",
      "itteration  26000 accracy tensor(96) loss 0.08147832006216049\n",
      "itteration  26500 accracy tensor(96) loss 0.0721246600151062\n",
      "itteration  27000 accracy tensor(96) loss 0.09396328777074814\n",
      "itteration  27500 accracy tensor(96) loss 0.11717608571052551\n",
      "itteration  28000 accracy tensor(96) loss 0.045461297035217285\n",
      "itteration  28500 accracy tensor(96) loss 0.09771537035703659\n",
      "itteration  29000 accracy tensor(96) loss 0.1278577446937561\n",
      "itteration  29500 accracy tensor(96) loss 0.03865399211645126\n",
      "itteration  30000 accracy tensor(96) loss 0.09647487848997116\n",
      "itteration  30500 accracy tensor(96) loss 0.06387748569250107\n",
      "itteration  31000 accracy tensor(96) loss 0.1081177145242691\n",
      "itteration  31500 accracy tensor(96) loss 0.0669129490852356\n",
      "itteration  32000 accracy tensor(97) loss 0.10854397714138031\n",
      "itteration  32500 accracy tensor(97) loss 0.10081911832094193\n",
      "itteration  33000 accracy tensor(97) loss 0.10462386161088943\n",
      "itteration  33500 accracy tensor(97) loss 0.09539230167865753\n",
      "itteration  34000 accracy tensor(97) loss 0.08166738599538803\n",
      "itteration  34500 accracy tensor(97) loss 0.08640256524085999\n",
      "itteration  35000 accracy tensor(97) loss 0.054084762930870056\n",
      "itteration  35500 accracy tensor(97) loss 0.027390137314796448\n",
      "itteration  36000 accracy tensor(97) loss 0.07891611754894257\n",
      "itteration  36500 accracy tensor(97) loss 0.09670600295066833\n",
      "itteration  37000 accracy tensor(97) loss 0.07413747161626816\n",
      "itteration  37500 accracy tensor(97) loss 0.053745388984680176\n",
      "itteration  38000 accracy tensor(97) loss 0.052256084978580475\n",
      "itteration  38500 accracy tensor(97) loss 0.07720554620027542\n",
      "itteration  39000 accracy tensor(97) loss 0.05719132348895073\n",
      "itteration  39500 accracy tensor(97) loss 0.050748538225889206\n",
      "itteration  40000 accracy tensor(97) loss 0.05262817069888115\n",
      "itteration  40500 accracy tensor(97) loss 0.07005492597818375\n",
      "itteration  41000 accracy tensor(97) loss 0.05248570069670677\n",
      "itteration  41500 accracy tensor(97) loss 0.046929869800806046\n",
      "itteration  42000 accracy tensor(97) loss 0.02738182060420513\n",
      "itteration  42500 accracy tensor(97) loss 0.048686765134334564\n",
      "itteration  43000 accracy tensor(97) loss 0.09167547523975372\n",
      "itteration  43500 accracy tensor(97) loss 0.06655080616474152\n",
      "itteration  44000 accracy tensor(97) loss 0.039192866533994675\n",
      "itteration  44500 accracy tensor(97) loss 0.026545582339167595\n",
      "itteration  45000 accracy tensor(97) loss 0.03907359763979912\n",
      "itteration  45500 accracy tensor(97) loss 0.05184631049633026\n",
      "itteration  46000 accracy tensor(97) loss 0.1182236298918724\n",
      "itteration  46500 accracy tensor(97) loss 0.045941539108753204\n",
      "itteration  47000 accracy tensor(97) loss 0.06335097551345825\n",
      "itteration  47500 accracy tensor(97) loss 0.05746077373623848\n",
      "itteration  48000 accracy tensor(97) loss 0.05056096985936165\n",
      "itteration  48500 accracy tensor(97) loss 0.05638812482357025\n",
      "itteration  49000 accracy tensor(97) loss 0.0571720227599144\n",
      "itteration  49500 accracy tensor(97) loss 0.028312869369983673\n",
      "itteration  50000 accracy tensor(97) loss 0.09756994992494583\n",
      "itteration  50500 accracy tensor(97) loss 0.014782214537262917\n",
      "itteration  51000 accracy tensor(97) loss 0.018703756853938103\n",
      "itteration  51500 accracy tensor(97) loss 0.07617548108100891\n",
      "itteration  52000 accracy tensor(97) loss 0.03831109032034874\n",
      "itteration  52500 accracy tensor(97) loss 0.03621305897831917\n",
      "itteration  53000 accracy tensor(97) loss 0.057151909917593\n",
      "itteration  53500 accracy tensor(97) loss 0.02368123084306717\n",
      "itteration  54000 accracy tensor(97) loss 0.03779130429029465\n",
      "itteration  54500 accracy tensor(97) loss 0.03264143317937851\n",
      "itteration  55000 accracy tensor(97) loss 0.04730505868792534\n",
      "itteration  55500 accracy tensor(97) loss 0.05518157035112381\n",
      "itteration  56000 accracy tensor(97) loss 0.09565263986587524\n",
      "itteration  56500 accracy tensor(97) loss 0.05924195423722267\n",
      "itteration  57000 accracy tensor(97) loss 0.047387849539518356\n",
      "itteration  57500 accracy tensor(97) loss 0.05561922490596771\n",
      "itteration  58000 accracy tensor(97) loss 0.03000706620514393\n",
      "itteration  58500 accracy tensor(97) loss 0.03893579542636871\n",
      "itteration  59000 accracy tensor(97) loss 0.025838108733296394\n",
      "itteration  59500 accracy tensor(97) loss 0.06430023163557053\n",
      "itteration  60000 accracy tensor(97) loss 0.014147710986435413\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(epochs):\n",
    "    for i, (images, lables) in enumerate(train_loader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, 28*28)).cuda()\n",
    "            lables = Variable(lables).cuda()\n",
    "        else:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            lables = Variable(lables)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_fun(outputs, lables)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        itter+=1\n",
    "        \n",
    "        if itter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, lables in test_loader:\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1,28*28)).cuda()\n",
    "                else:\n",
    "                    images = Variable(images.view(-1,28*28))\n",
    "                    \n",
    "                outputs = model(images)\n",
    "                \n",
    "                _, predicts = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += lables.size(0)\n",
    "                \n",
    "                correct += (predicts.cpu() == lables.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * correct/ total\n",
    "            \n",
    "            print('itteration ', itter, 'accracy', accuracy, 'loss', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
